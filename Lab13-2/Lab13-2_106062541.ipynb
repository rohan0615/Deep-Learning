{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict = np.load('./dataset/enVocab.npy').tolist()\n",
    "idx_dict = np.load('./dataset/enRev.npy').tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，先把英文的 dictionary load 進來，就能將單字轉成 ID；同時也把 decoder load 進來，就能將 ID 轉回單字。  \n",
    "原本想嘗試自己建構 dictionary，不過有同學說 memory 會不夠，因此我也就沒有多做嘗試。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuations = ['.', ',', '!', '?', '-', '_', '\\\"']\n",
    "data = []\n",
    "\n",
    "with open('./dataset/conversations.txt', 'r', encoding='utf-8') as text:\n",
    "    lines = text.readlines()\n",
    "    \n",
    "    for lines in lines:\n",
    "        lines = lines.strip()\n",
    "        for punc in punctuations:\n",
    "            if punc in lines:\n",
    "                lines = lines.replace(punc, '')\n",
    "        words = lines.split()\n",
    "        words = [w for w in words if w]\n",
    "        \n",
    "        if words:\n",
    "            data.append(words)\n",
    "    text.close()\n",
    "    del lines\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can', 'we', 'make', 'this', 'quick', 'roxanne', 'korrine', 'and', 'andrew', 'barrett', 'are', 'having', 'an', 'incredibly', 'horrendous', 'public', 'break', 'up', 'on', 'the', 'quad', 'again']\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著要對 conversations 做 preprocessing。  \n",
    "1. 我先去掉讀進來的句子頭尾的空白。  \n",
    "2. 先把句子中的標點符號去掉。\n",
    "3. 把句子切成一個個單字的 list，再把裡面的空的單字去掉。\n",
    "4. 最後，如果是有單字的句子我才把他加進最後的 data 中，避免有空的句子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著，將 data 中的 conversations 分成提問和回答。  \n",
    "我的作法是，把單數的句子當作提問，雙數的句子當作回應。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question = []\n",
    "ans = []\n",
    "for idx, line in enumerate(data):\n",
    "    tmp = []\n",
    "    tmp.append(word_dict['<BEG>'])# add <BEG>\n",
    "    for word in line:\n",
    "        if word in word_dict:\n",
    "            tmp.append(word_dict[word])\n",
    "    \n",
    "    if idx%2 == 0:# for question\n",
    "        tmp.append(word_dict['<END>'])# add <END>\n",
    "        question.append(tmp)\n",
    "    elif idx%2 == 1:# for ans\n",
    "        tmp.append(word_dict['<END>'])# add <END>\n",
    "        ans.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再來，要考慮到 max length。  \n",
    "根據我的實驗，太長會讓我的 RNN 失敗，所以我想要限制住每一句的 max length，如果大於 max_length 我就只取到 max_length。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions max length: 468\n",
      "Ans max length: 309\n"
     ]
    }
   ],
   "source": [
    "question_max_length = sorted([len(question[i]) for i in range(len(question))], reverse=True)[0]\n",
    "ans_max_length = sorted([len(ans[i]) for i in range(len(ans))], reverse=True)[0]\n",
    "print('Questions max length:', question_max_length)\n",
    "print('Ans max length:', ans_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_length = 60 # hyperparameter\n",
    "\n",
    "question_short = []\n",
    "ans_short = []\n",
    "\n",
    "for i in range(len(ans)):\n",
    "    if len(question[i]) > max_length:\n",
    "        question_short.append(question[i][:max_length])\n",
    "    else:\n",
    "        question_short.append(question[i])\n",
    "        \n",
    "    if len(ans[i]) > max_length:\n",
    "        ans_short.append(ans[i][:max_length])\n",
    "    else:\n",
    "        ans_short.append(ans[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions max length: 60\n",
      "Ans max length: 60\n"
     ]
    }
   ],
   "source": [
    "new_question_max_length = sorted([len(question_short[i]) for i in range(len(question_short))], reverse=True)[0]\n",
    "new_ans_max_length = sorted([len(ans_short[i]) for i in range(len(ans_short))], reverse=True)[0]\n",
    "print('Questions max length:', new_question_max_length)\n",
    "print('Ans max length:', new_ans_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch size 的部分，如果我用 256 的話會完全跑不起來，所以我最後只使用128。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    def __init__(self, en_corpus, ch_corpus, en_pad, ch_pad, en_max_len, ch_max_len, batch_size):\n",
    "        #self.n = len(en_corpus)\n",
    "        self.batch_num = len(en_corpus) // batch_size\n",
    "        self.n = self.batch_num * batch_size\n",
    "\n",
    "        self.en_corpus = en_corpus\n",
    "        self.ch_corpus = ch_corpus\n",
    "\n",
    "        self.en_pad = en_pad\n",
    "        self.ch_pad = ch_pad\n",
    "\n",
    "        self.en_max_len = en_max_len\n",
    "        self.ch_max_len = ch_max_len\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.xs = [np.zeros(self.n, dtype=np.int32) for _ in range(en_max_len)]  # encoder inputs\n",
    "        self.ys = [np.zeros(self.n, dtype=np.int32) for _ in range(ch_max_len)]  # decoder inputs\n",
    "        self.gs = [np.zeros(self.n, dtype=np.int32) for _ in range(ch_max_len)]  # decoder outputs\n",
    "        self.ws = [np.zeros(self.n, dtype=np.float32) for _ in range(ch_max_len)]  # decoder weight for loss caculation\n",
    "        for b in range(self.batch_num):\n",
    "            for i in range(b * batch_size, (b + 1) * batch_size):\n",
    "                for j in range(len(en_corpus[i]) - 2):\n",
    "                    self.xs[j][i] = en_corpus[i][j + 1]\n",
    "                for j in range(j + 1, en_max_len):\n",
    "                    self.xs[j][i] = en_pad\n",
    "\n",
    "                for j in range(len(ch_corpus[i]) - 1):\n",
    "                    self.ys[j][i] = ch_corpus[i][j]\n",
    "                    self.gs[j][i] = ch_corpus[i][j + 1]\n",
    "                    self.ws[j][i] = 1.0\n",
    "                for j in range(j + 1, ch_max_len):  # don't forget padding and let loss weight zero\n",
    "                    self.ys[j][i] = ch_pad\n",
    "                    self.gs[j][i] = ch_pad\n",
    "                    self.ws[j][i] = 0.0      \n",
    "    def get(self, batch_id):\n",
    "        x = [ self.xs[i][batch_id * self.batch_size:(batch_id + 1) * self.batch_size] for i in range(self.en_max_len) ]\n",
    "        y = [ self.ys[i][batch_id * self.batch_size:(batch_id + 1) * self.batch_size] for i in range(self.ch_max_len) ]\n",
    "        g = [ self.gs[i][batch_id * self.batch_size:(batch_id + 1) * self.batch_size] for i in range(self.ch_max_len) ]\n",
    "        w = [ self.ws[i][batch_id * self.batch_size:(batch_id + 1) * self.batch_size] for i in range(self.ch_max_len) ]\n",
    "        return x, y, g, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "製作 batch 的方法基本上差不多。  \n",
    "比較值得提到的是我在這邊沒有碰到記憶體不足的問題，可以直接在 initial 時把所有的結果都做好，並在 call get 時把需要的 batch 直接擷取出來就好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<BEG> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<END> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "i know the a lie <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<BEG> tell me something true <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "tell me something true <END> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = BatchGenerator(question_short, ans_short, word_dict['<PAD>'], word_dict['<PAD>'], max_length, max_length, BATCH_SIZE)\n",
    "x, y, g, w = batch.get(2)\n",
    "for i in range(2):\n",
    "    print(' '.join([idx_dict[x[j][i]] for j in range(max_length)]))\n",
    "    print(' '.join([idx_dict[y[j][i]] for j in range(max_length)]))\n",
    "    print(' '.join([idx_dict[g[j][i]] for j in range(max_length)]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ChatBox:\n",
    "\n",
    "    def __init__(self, en_max_len, ch_max_len, en_size, ch_size):\n",
    "        self.en_max_len = en_max_len\n",
    "        self.ch_max_len = ch_max_len\n",
    "\n",
    "        with tf.variable_scope('seq2seq_input/output'):\n",
    "            self.enc_inputs = [ tf.placeholder(tf.int32, [None]) for i in range(en_max_len) ]  # time mojor feed\n",
    "            self.dec_inputs = [ tf.placeholder(tf.int32, [None]) for i in range(ch_max_len) ]\n",
    "            self.groundtruths = [ tf.placeholder(tf.int32, [None]) for i in range(ch_max_len) ]\n",
    "            self.weights = [ tf.placeholder(tf.float32, [None]) for i in range(ch_max_len) ]\n",
    "\n",
    "        with tf.variable_scope('seq2seq_rnn'):  # training by teacher forcing\n",
    "            self.out_cell = tf.contrib.rnn.LSTMCell(512)\n",
    "            self.outputs, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                  self.enc_inputs, self.dec_inputs, self.out_cell, en_size, ch_size, 300)\n",
    "        with tf.variable_scope('seq2seq_rnn', reuse=True):  # predict by feeding previous\n",
    "            self.pred_cell = tf.contrib.rnn.LSTMCell(512, reuse=True)  # reuse cell for train and test\n",
    "            self.predictions, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                  self.enc_inputs,\n",
    "                  self.dec_inputs,\n",
    "                  self.pred_cell,\n",
    "                  en_size,\n",
    "                  ch_size,\n",
    "                  300,\n",
    "                  feed_previous=True)\n",
    "        with tf.variable_scope('loss'):\n",
    "          # caculate weighted loss\n",
    "            self.loss = tf.reduce_mean(tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "                  self.outputs, self.groundtruths, self.weights))\n",
    "            self.optimizer = tf.train.AdamOptimizer(0.002).minimize(self.loss)\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def train(self, x, y, g, w):\n",
    "        fd = {}\n",
    "        for i in range(self.en_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i]  # show how to feed a list\n",
    "\n",
    "        for i in range(self.ch_max_len):\n",
    "            fd[self.dec_inputs[i]] = y[i]\n",
    "            fd[self.groundtruths[i]] = g[i]\n",
    "            fd[self.weights[i]] = w[i]\n",
    "\n",
    "        loss, _ = self.sess.run([self.loss, self.optimizer], fd)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def output(self, x, y):\n",
    "        fd = {}\n",
    "        for i in range(self.en_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i]\n",
    "\n",
    "        for i in range(self.ch_max_len):\n",
    "            fd[self.dec_inputs[i]] = y[i]\n",
    "\n",
    "        out = self.sess.run(self.outputs, fd)\n",
    "        return out\n",
    "\n",
    "    def predict(self, x, ch_beg):\n",
    "        fd = {}\n",
    "        for i in range(self.en_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i]\n",
    "\n",
    "        for i in range(self.ch_max_len):  # when feed previous, the fist token should be '<BEG>', and others are useless\n",
    "            if i == 0:\n",
    "                fd[self.dec_inputs[i]] = np.ones(x[i].shape, dtype=np.int32) * ch_beg\n",
    "            else:\n",
    "                fd[self.dec_inputs[i]] = np.zeros(x[i].shape, dtype=np.int32)\n",
    "\n",
    "        pd = self.sess.run(self.predictions, fd)\n",
    "\n",
    "        return pd\n",
    "\n",
    "    def save(self, e):\n",
    "        self.saver.save(self.sess, './model/chatbox/chatbox%d.ckpt' % (e + 1))\n",
    "\n",
    "    def restore(self, e):\n",
    "        self.saver.restore(self.sess, './model/chatbox/chatbox%d.ckpt' % (e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = ChatBox(max_length, max_length, len(word_dict), len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = BatchGenerator(question_short, ans_short, word_dict['<PAD>'], word_dict['<PAD>'], max_length, max_length, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Epoch:  1\n",
      "#Epoch:  2\n",
      "#Epoch:  3\n",
      "#Epoch:  4\n",
      "#Epoch:  5\n",
      "#Epoch:  6\n",
      "#Epoch:  7\n",
      "#Epoch:  8\n",
      "#Epoch:  9\n",
      "#Epoch:  10\n",
      "#Epoch:  11\n",
      "#Epoch:  12\n",
      "#Epoch:  13\n",
      "#Epoch:  14\n",
      "#Epoch:  15\n",
      "#Epoch:  16\n",
      "#Epoch:  17\n",
      "#Epoch:  18\n",
      "#Epoch:  19\n",
      "#Epoch:  20\n",
      "#Epoch:  21\n",
      "#Epoch:  22\n",
      "#Epoch:  23\n",
      "#Epoch:  24\n",
      "#Epoch:  25\n",
      "#Epoch:  26\n",
      "#Epoch:  27\n",
      "#Epoch:  28\n",
      "#Epoch:  29\n",
      "#Epoch:  30\n",
      "#Epoch:  31\n",
      "#Epoch:  32\n",
      "#Epoch:  33\n",
      "#Epoch:  34\n",
      "#Epoch:  35\n",
      "#Epoch:  36\n",
      "#Epoch:  37\n",
      "#Epoch:  38\n",
      "#Epoch:  39\n",
      "#Epoch:  40\n",
      "Wall time: 18h 21min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rec_loss = []\n",
    "batch_num = len(ans_short)//BATCH_SIZE\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    for e in range(EPOCHS):\n",
    "        train_loss = 0\n",
    "        print('#Epoch: ', e+1)\n",
    "        for b in range(batch_num):\n",
    "            x, y, g, w = batch.get(b)\n",
    "            batch_loss = model.train(x, y, g, w)\n",
    "            train_loss += batch_loss\n",
    "\n",
    "        train_loss /= batch_num\n",
    "        rec_loss.append(train_loss)\n",
    "        model.save(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./model/chatbox/rec_loss.npy', rec_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1370b051a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rec_loss = np.load('./model/chatbox/rec_loss.npy')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt_loss = plt.plot([rec_loss[i] for i in range(len(rec_loss))])\n",
    "plt.legend(['Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "經過 40 個 epochs，從 learning curve 可以發現 loss 有 converge。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cherry pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "def cherry_pick(records, n, upper_bound=1.0):\n",
    "    bleus = []\n",
    "\n",
    "    for en, ch_gr, ch_pd in records:\n",
    "        bleu = nltk.translate.bleu_score.sentence_bleu(\n",
    "            [ch_gr], ch_pd)  # caculate BLEU by nltk\n",
    "        bleus.append(bleu)\n",
    "\n",
    "    lst = [i for i in range(len(records)) if bleus[i] <= upper_bound]\n",
    "    lst = sorted(lst, key=lambda i: bleus[i], reverse=True)  # sort by BLEU score\n",
    "\n",
    "    return [records[lst[i]] for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what think of the\n",
      "nice voice\n",
      "nice voice\n",
      "\n",
      "and you must be look the since wild bill they say what do you think should i hate him\n",
      "age\n",
      "age\n",
      "\n",
      "enough never\n",
      "wrong\n",
      "wrong\n",
      "\n",
      "i wonder how that\n",
      "turn it down\n",
      "turn it down\n",
      "\n",
      "what did you do to those people\n",
      "nothing\n",
      "nothing\n",
      "\n",
      "to experience beyond anything ever known at least what i was promised when i bought it pleasure from heaven or hell i much care which\n",
      "hell\n",
      "hell\n",
      "\n",
      "heard you had quite a night\n",
      "yeah\n",
      "yeah\n",
      "\n",
      "no i have only activated your cognitive and communication\n",
      "why\n",
      "why\n",
      "\n",
      "it appears to from us\n",
      "from enterprise\n",
      "from enterprise\n",
      "\n",
      "what about\n",
      "forget him\n",
      "forget about the\n",
      "\n",
      "listen not like he makes you feel like you belong and good it really is i wish i could do something about this but i i can promise you only two things always look this good i give up on you ever\n",
      "i like that\n",
      "i guess i can\n",
      "\n",
      "i guess i kind of lost sight of things may the wind always be at your back and the sun always upon your face and the of carry you to dance with the stars love george\n",
      "that was a beautiful message\n",
      "so you could be a little\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohan\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\Rohan\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\Rohan\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import random as rd\n",
    "\n",
    "records = []\n",
    "\n",
    "for i in range(10):\n",
    "    i = rd.randint(0, batch_num - 1)  # random pick one to translate\n",
    "\n",
    "    x, y, g, w = batch.get(i)\n",
    "    out = model.output(x, y)\n",
    "    pd = model.predict(x, word_dict['<BEG>'])\n",
    "    \n",
    "    for j in range(10):\n",
    "        j = rd.randint(0, BATCH_SIZE - 1)\n",
    "\n",
    "        en = [idx_dict[x[i][j]] for i in range(max_length)]\n",
    "        en = en[:en.index('<PAD>')]\n",
    "        ch_gr = [idx_dict[g[i][j]] for i in range(max_length)]\n",
    "        if '<END>' in ch_gr:\n",
    "            ch_gr = ch_gr[:ch_gr.index('<END>')]\n",
    "        ch_pd = [idx_dict[np.argmax(pd[i][j, :])] for i in range(max_length)]\n",
    "        if '<END>' in ch_pd:\n",
    "            ch_pd = ch_pd[:ch_pd.index('<END>')]\n",
    "\n",
    "        records.append([en, ch_gr, ch_pd])\n",
    "\n",
    "n = 12  # how many result we show\n",
    "rec_cherry = cherry_pick(records, n)\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(3):\n",
    "        print(' '.join(rec_cherry[i][j]))\n",
    "\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
